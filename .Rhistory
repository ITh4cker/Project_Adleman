training = adData[ inTrain,]
testing = adData[-inTrain,]
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.90)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training$diagnosis ~., method = "glm", data = trainPC)
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.90)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training$diagnosis ~., method = "glm", data = trainPC)
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.80)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training$diagnosis ~., method = "glm", data = trainPC)
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.80)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training$diagnosis ~., method = "glm", data = trainPC)
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.80)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training$diagnosis ~., method = "glm", data = trainPC)
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
availablePackages <- available.packages()[,1]
install.packages(availablePackages)
installedPackages <- .packages(all.available = TRUE)
install.packages('e1071', dependencies=TRUE)
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.90)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training2$diagnosis ~., method = "glm", data = trainPC)
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
library(caret)
library(AppliedPredictiveModeling)
library(e1071)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training2 <- training[,c(1,grep("^IL", colnames(training)))]
testing2 <- testing[,c(1,grep("^IL", colnames(testing)))]
preProc <- preProcess(training2[,-1], method = "pca", thresh = 0.90)
trainPC <- predict(preProc, training2[-1])
modelFit <- train(training2$diagnosis ~., method = "glm", data = trainPC)
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
confusionMatrix(testing2$diagnosis, predict(modelFit, testing2))
sessionInfo()
library(shiny)
deployApp()
library(shinyapps)
deployApp()
deployApp()
library(devtools)
devtools::install_github('rstudio/shinyapps')
library(shinyapps)
deployApp()
shiny::runApp('Documents/GitHub/Shiny_Application')
deployApp()
createmean <- function(x) {
answer <- mean(x)
return(answer)
}
x<- 1:10
createmean(x)
class(createmean(x))
class(c)
class(x)
showMethods("lm")
showMethods("colSums")
showMethods("show")
show
lm
colSums
dgamma
show
getMethod("./config/global_temp.R")
?purl
words <- c("a","b","c","d","e")
n1 <- c(rep("a",50),rep("b",50))
set.seed(3535)
n2 <- sample(words, 100, replace = TRUE)
n3 <- sample(words, 100, replace = TRUE)
n4 <- sample(words, 100, replace = TRUE)
ngrams <- paste(n1,n2,n3,n4)
print(ngrams)
paste("a", "b", "c")
print("100 total entries")
print(paste(length(unique(ngrams)),"unique total entries"))
print(paste("Of those, ", length(tapply(ngrams,ngrams,length)[tapply(ngrams,ngrams,length) >1]), " account for ", sum(tapply(ngrams,ngrams,length)[tapply(ngrams,ngrams,length) >1]), " of the total"))
tapply(ngrams,ngrams,length)[tapply(ngrams,ngrams,length) >1]
ngrams <- paste(n2,n3,n4)
print(paste(length(unique(ngrams)),"unique total entries"))
print(paste("Of those, ", length(tapply(ngrams,ngrams,length)[tapply(ngrams,ngrams,length) >1]), " account for ", sum(tapply(ngrams,ngrams,length)[tapply(ngrams,ngrams,length) >1]), " of the total"))
tapply(ngrams,ngrams,length)[tapply(ngrams,ngrams,length) >1]
grams <- paste(n2,n3)
print("100 total entries")
table(ngrams, n4)
ngrams <- paste(n1[n2=="c" & n3=="b"], n2[n2=="c" & n3=="b"],n3[n2=="c" & n3=="b"])
print("100 total entries")
table(ngrams, n4[n2=="c" & n3=="b"])
# Goal here to come up wtih three sets per data source
load("./cache/docs_stemmed.Rdata")
blogs <- data.frame(docs[[1]]$content, stringsAsFactors = FALSE)
colnames(blogs)[1] <- "blog_data"
news <- data.frame(docs[[2]]$content, stringsAsFactors = FALSE)
colnames(news)[1] <- "news_data"
twitter <- data.frame(docs[[3]]$content, stringsAsFactors = FALSE)
colnames(twitter)[1] <- "twitter_data"
docs <- NULL
source("./helpers/data_seperator.R")
blogs$Type <- ""
news$Type <- ""
twitter$Type <- ""
# Splitting Blogs
set.seed(112)
blogs$Type <- lapply(blogs$Type, FUN = data_seperator)
news$Type <- lapply(news$Type, FUN = data_seperator)
twitter$Type <- lapply(twitter$Type, FUN = data_seperator)
test_blogs <- blogs[which(blogs$Type=="Test"),]
train_blogs <- blogs[which(blogs$Type=="Train"),]
CV_blogs <- blogs[which(blogs$Type=="CV"),]
test_news <- news[which(news$Type=="Test"),]
train_news <- news[which(news$Type=="Train"),]
CV_news <- news[which(news$Type=="CV"),]
test_twitter <- twitter[which(twitter$Type=="Test"),]
train_twitter <- twitter[which(twitter$Type=="Train"),]
CV_twitter <- twitter[which(twitter$Type=="CV"),]
# Removing unnecessary data
test_blogs$Type <- NULL
train_blogs$Type <- NULL
CV_blogs$Type <- NULL
blogs <- NULL
test_news$Type <- NULL
train_news$Type <- NULL
CV_news$Type <- NULL
news <- NULL
test_twitter$Type <- NULL
train_twitter$Type <- NULL
CV_twitter$Type <- NULL
twitter <- NULL
# Saving data
save(test_blogs,file = "./cache/test_blogs.RData")
save(train_blogs,file = "./cache/train_blogs.RData")
save(CV_blogs,file = "./cache/CV_blogs.RData")
save(test_news,file = "./cache/test_news.RData")
save(train_news,file = "./cache/train_news.RData")
save(CV_news,file = "./cache/CV_news.RData")
save(test_twitter,file = "./cache/test_twitter.RData")
save(train_twitter,file = "./cache/train_twitter.RData")
save(CV_twitter,file = "./cache/CV_twitter.RData")
# Clearning space
test_blogs <- NULL
train_blogs <- NULL
CV_blogs <- NULL
test_news <- NULL
train_news <- NULL
CV_news <- NULL
test_twitter <- NULL
train_twitter <- NULL
CV_twitter <- NULL
# Goal here to come up wtih three sets per data source
load("./cache/docs_stemmed.Rdata")
blogs <- data.frame(docs[[1]]$content, stringsAsFactors = FALSE)
colnames(blogs)[1] <- "blog_data"
news <- data.frame(docs[[2]]$content, stringsAsFactors = FALSE)
colnames(news)[1] <- "news_data"
twitter <- data.frame(docs[[3]]$content, stringsAsFactors = FALSE)
colnames(twitter)[1] <- "twitter_data"
docs <- NULL
source("./helpers/data_seperator.R")
blogs$Type <- ""
news$Type <- ""
twitter$Type <- ""
# Splitting Blogs
set.seed(112)
blogs$Type <- lapply(blogs$Type, FUN = data_seperator)
news$Type <- lapply(news$Type, FUN = data_seperator)
twitter$Type <- lapply(twitter$Type, FUN = data_seperator)
test_blogs <- blogs[which(blogs$Type=="Test"),]
train_blogs <- blogs[which(blogs$Type=="Train"),]
CV_blogs <- blogs[which(blogs$Type=="CV"),]
test_news <- news[which(news$Type=="Test"),]
train_news <- news[which(news$Type=="Train"),]
CV_news <- news[which(news$Type=="CV"),]
test_twitter <- twitter[which(twitter$Type=="Test"),]
train_twitter <- twitter[which(twitter$Type=="Train"),]
CV_twitter <- twitter[which(twitter$Type=="CV"),]
# Removing unnecessary data
test_blogs$Type <- NULL
train_blogs$Type <- NULL
CV_blogs$Type <- NULL
blogs <- NULL
test_news$Type <- NULL
train_news$Type <- NULL
CV_news$Type <- NULL
news <- NULL
test_twitter$Type <- NULL
train_twitter$Type <- NULL
CV_twitter$Type <- NULL
twitter <- NULL
# Saving data
save(test_blogs,file = "./cache/test_blogs.RData")
save(train_blogs,file = "./cache/train_blogs.RData")
save(CV_blogs,file = "./cache/CV_blogs.RData")
save(test_news,file = "./cache/test_news.RData")
save(train_news,file = "./cache/train_news.RData")
save(CV_news,file = "./cache/CV_news.RData")
save(test_twitter,file = "./cache/test_twitter.RData")
save(train_twitter,file = "./cache/train_twitter.RData")
save(CV_twitter,file = "./cache/CV_twitter.RData")
# Clearning space
test_blogs <- NULL
train_blogs <- NULL
CV_blogs <- NULL
test_news <- NULL
train_news <- NULL
CV_news <- NULL
test_twitter <- NULL
train_twitter <- NULL
CV_twitter <- NULL
2 +2
install.packages("ggplot2")
library(ggplot2)
subset(wf, freq>500) %>%
ggplot(aes(word, freq1_docs)) +
geom_bar(stat="identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
wf <- data.frame(word=names(freq1_docs), freq=freq1_docs)
availablePackages <- available.packages()[,1]
install.packages(availablePackages)
?gvlma
library(MASS)
data(UScrime)
head(UScrime)
View(UScrime)
t.test(Prob ~ So, data=UScrime)
t.test(Prob,So)
t.test(UScrim$Prob,UScrime$So)
t.test(UScrime$Prob,UScrime$So)
library(multcomp)
levels(cholesterol$trt)
data(colestrol)
data(cholesterol)
head(cholesterol)
fit.aov <- aov(response ~ trt, data=cholesterol)
summary(fit.aov)
library(pwr)
pwr.t.test(d=.8, sig.level=.05, power=.9, type="two.sample", alternative="two.sided")
install.packages('neuralnet')
1+`
1+1
install.packages('neuralnet')
library("neuralnet")
install.packages('neuralnet')
library("neuralnet")
traininginput <-  as.data.frame(runif(50, min=0, max=100))
trainingoutput <- sqrt(traininginput)
View(trainingoutput)
View(traininginput)
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have 10 hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)
print(net.sqrt)
View(trainingdata)
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)
print(net.sqrt)
plot(net.sqrt)
testdata <- as.data.frame((1:10)^2) #Generate some squared numbers
net.results <- compute(net.sqrt, testdata) #Run them through the neural network
ls(net.results)
print(net.results$net.result)
cleanoutput <- cbind(testdata,sqrt(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=[10,10], threshold=0.01)
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=(3,2,1), threshold=0.01)
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=20, threshold=0.01)
print(net.sqrt)
plot(net.sqrt)
?neuralnet
net.sqrt$net.result
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
library("neuralnet")
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
View(binary.data)
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
View(xor.data)
plot(net.xor, rep="best")
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
Var1 <- rpois(100,0.5)
Var2 <- rbinom(100,2,0.6)
Var3 <- rbinom(100,1,0.5)
SUM <- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data <- data.frame(Var1+Var2+Var3, SUM)
print(net.sum <- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1,
act.fct="tanh"))
prediction(net.sum)
8*5*4*12
8*5*50
16*5*50
16*7*50
16*7*50/(16*7*50+16*5*50)
0.6*12
12-7.2
View(infert)
str(inferet)
str(inferet)
inferet
str(infert)
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
View(infert)
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
?print
summary(net.inferet)
summary(net.infert)
install.packages('neuralnet')
library("neuralnet")
#Going to create a neural network to perform sqare rooting
#Type ?neuralnet for more information on the neuralnet library
#Generate 50 random numbers uniformly distributed between 0 and 100
#And store them as a dataframe
traininginput <-  as.data.frame(runif(50, min=0, max=100))
trainingoutput <- sqrt(traininginput)
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have 10 hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)
print(net.sqrt)
#Plot the neural network
plot(net.sqrt)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)^2) #Generate some squared numbers
net.results <- compute(net.sqrt, testdata) #Run them through the neural network
#Lets see what properties net.sqrt has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,sqrt(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
#install.packages('neuralnet')
library("neuralnet")
# Regular implimentation of a Neral Network
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")
# This is a
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
Var1 <- rpois(100,0.5)
Var2 <- rbinom(100,2,0.6)
Var3 <- rbinom(100,1,0.5)
SUM <- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data <- data.frame(Var1+Var2+Var3, SUM)
print(net.sum <- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1,
act.fct="tanh"))
prediction(net.sum)
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
exp(2)
exp((100-102/2))
exp((100 − 102)/2)
exp((100−102)/2)
exp((100−100)/2)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE)
)
summary(net)
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
?AIC
AIC(net)
AIC(net.infert)
AIC(net.sqrt)
# Working directory initialization
wd <- "./Documents/GitHub/Project_Adleman"
setwd(wd)
library(rattle)
library(tm)
library(SnowballC)
library(caret)
library(plyr)
library(tau)
library(ngram)
library(knitr)
library(Matrix)
library(RWeka)
library(ggplot2)
library(reshape)
library(wordcloud)
# Cleaners
wd <- NULL
getwd()
?read.table
getwd()
file1Meta <- read.table(file = "./data/0A32eTdBKayjCWhZqDOQ.asm")
file1Meta <- read.table(file = "./data/0A32eTdBKayjCWhZqDOQ.asm", header = T)
file1Meta <- read.table(file = "./data/0A32eTdBKayjCWhZqDOQ.bytes", header = F)
File1Meta <- NULL
file1Meta <- NULL
file1Binary <- read.table(file = "./data/0A32eTdBKayjCWhZqDOQ.bytes", header = F)
View(file1Binary)
file1Meta <- read.table(file = "./data/0A32eTdBKayjCWhZqDOQ.asm", sep="\t")
file1Meta <- read.table(file = "./data/0A32eTdBKayjCWhZqDOQ.asm", sep=" ")
